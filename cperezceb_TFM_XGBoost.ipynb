{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">TFM - Previsión de demanda mediante uso de técnicas de machine learning\n",
    "</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Carlos Pérez Cebrián</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) es una versión optimizada y escalable del algoritmo de gradient boosting, diseñada para velocidad, precisión y eficiencia. Combina múltiples modelos débiles, típicamente árboles de decisión, para crear un modelo más fuerte. XGBoost se destaca por su eficiencia computacional y rendimiento.\n",
    "\n",
    "Es ideal para tareas que requieren grandes volúmenes de datos y alta precisión, especialmente en clasificación y regresión, utilizando ensemble learning y gradient boosting para minimizar errores.\n",
    "\n",
    "Ensemble learning: combinación de múltiples modelos para obtener un modelo más robusto y preciso. La idea es que al combinar varios modelos, se pueden compensar los errores individuales de cada uno. \n",
    "\n",
    "Boosting: Se entrenan modelos secuencialmente, donde cada modelo intenta corregir los errores del modelo anterior. \n",
    "\n",
    "Gradient boosting es una técnica de boosting que se enfoca en minimizar los errores de predicción mediante la optimización del gradiente. Imagina que tienes un conjunto de datos y un modelo inicial que comete ciertos errores. Con cada iteración de gradient boosting, se añade un nuevo modelo que corrige esos errores, reduciendo progresivamente la cantidad de error total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las siguientes librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import cluster        # algoritmos de clustering.\n",
    "from sklearn import datasets       \n",
    "from sklearn.preprocessing import StandardScaler #estandarizar datos\n",
    "from sklearn.decomposition import PCA #análisis de componentes principales, reducir dimensiones\n",
    "from sklearn.preprocessing import LabelEncoder #codificación variables categóricas\n",
    "from sklearn.model_selection import train_test_split  #split dataframes train, test, validación\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score #metricas\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor   # Importar la clase XGBClassifier\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar el conjunto de datos:\n",
    "(Nota: el conjunto de datos es el resultado de una fase de exploración, analisis y preparación previa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   producto idSecuencia  udsVenta  bolOpen  EnPromocion  anyomes  diasemana  \\\n",
      "0         1  2022-12-06  0.000000        1            0        2          2   \n",
      "1         2  2022-12-06  1.098612        1            0        2          2   \n",
      "2         3  2022-12-06  0.000000        1            0        2          2   \n",
      "3         4  2022-12-06  0.000000        1            0        2          2   \n",
      "4         5  2022-12-06  0.000000        1            0        2          2   \n",
      "\n",
      "   semana  media_7_dias  media_30_dias  venta_lag_1  venta_lag_7  venta_lag_30  \n",
      "0      49      2.120606       2.301086     2.833213     2.079442           0.0  \n",
      "1      49      1.874395       2.042219     2.079442     1.791759           0.0  \n",
      "2      49      1.844732       1.681282     2.302585     2.564949           0.0  \n",
      "3      49      1.186639       1.540582     1.098612     0.000000           0.0  \n",
      "4      49      1.571729       1.871285     2.302585     2.564949           0.0  \n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV con los datos de ventas\n",
    "file_path = \"Data/VentasRdo.csv\"\n",
    "file_path = \"Data/VentasRdo_10productos.csv\"\n",
    "df = pd.read_csv(file_path, parse_dates=['idSecuencia'])\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df[['producto', 'bolOpen', 'EnPromocion', 'diasemana', 'semana', 'media_7_dias', 'media_30_dias', 'venta_lag_1', 'venta_lag_7', 'venta_lag_30']].copy()  # Características\n",
    "y = df['udsVenta']  # Variable objetivo\n",
    "\n",
    "# Codificación de las variables categóricas\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Modificar las columnas\n",
    "X['producto'] = label_encoder.fit_transform(X['producto'])\n",
    "X['bolOpen'] = label_encoder.fit_transform(X['bolOpen'])\n",
    "X['EnPromocion'] = label_encoder.fit_transform(X['EnPromocion'])\n",
    "X['diasemana'] = label_encoder.fit_transform(X['diasemana'])\n",
    "X['semana'] = label_encoder.fit_transform(X['semana'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (5608, 10), X_test: (1402, 10), y_train: (5608,), y_test: (1402,)\n"
     ]
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Dimensiones de X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a un formato compatible con XGBoost (DMatrix)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros del modelo XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Para regresión (predecir un valor continuo)\n",
    "    'eval_metric': 'rmse',  # Métrica de evaluación: Root Mean Squared Error (RMSE)\n",
    "    'eta': 0.1,  # Tasa de aprendizaje\n",
    "    'max_depth': 6,  # Profundidad máxima de los árboles\n",
    "    'colsample_bytree': 0.8,  # Proporción de características a considerar por árbol\n",
    "    'subsample': 0.8,  # Proporción de muestra para cada árbol\n",
    "    'nthread': 4  # Número de hilos (ajustar a núcleos de CPU)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo\n",
    "num_round = 100  # Número de iteraciones\n",
    "modelo_xgb = xgb.train(params, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir sobre test\n",
    "y_pred = modelo_xgb.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir sobre train\n",
    "y_train_pred = modelo_xgb.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7018889632416443\n",
      "MSE: 0.4926481167204303\n",
      "MAE: 0.5387957978632635\n",
      "R²: 0.5255455479291898\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7018889632416443\n",
      "MSE: 16.307085955163657\n",
      "MAE: 0.5387957978632635\n",
      "R²: 0.40166687608585394\n"
     ]
    }
   ],
   "source": [
    "y_pred_original = np.exp(y_pred)  # Revertir la transformación\n",
    "y_test_original = np.exp(y_test)\n",
    "\n",
    "rmse_original = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar modelo\n",
    "modelo_xgb.save_model('modelo_xgboost.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otra version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ajustar modelo\n",
    "modelo_xgbregresion = XGBRegressor() #modelo de regresión\n",
    "modelo_xgbregresion.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo_xgbregresion.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7426122972150172\n",
      "MSE: 0.5514730239749651\n",
      "MAE: 0.569079576942964\n"
     ]
    }
   ],
   "source": [
    "# Métricas\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.204137800257273\n",
      "MSE: 17.674774643552066\n",
      "MAE: 2.8893040452415084\n",
      "R²: 0.3514841857072366\n"
     ]
    }
   ],
   "source": [
    "y_pred_original = np.exp(y_pred)  # Revertir la transformación\n",
    "y_test_original = np.exp(y_test)\n",
    "\n",
    "rmse_original = mean_squared_error(y_test_original, y_pred_original, squared=False)\n",
    "mse_original = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2_original = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"RMSE: {rmse_original}\")\n",
    "print(f\"MSE: {mse_original}\")\n",
    "print(f\"MAE: {mae_original}\")\n",
    "print(f\"R²: {r2_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE (1.99): En promedio, las predicciones están a 1.99 unidades de las reales. Este valor refleja cómo de \"severa\" es la desviación promedio, con penalizaciones más altas para los errores grandes.\n",
    "MSE (3.96): El error cuadrado promedio es de 3.96 unidades al cuadrado. Aunque útil para ver el impacto de los errores grandes, no es directamente interpretable en las mismas unidades que la variable objetivo.\n",
    "MAE (1.33): En promedio, las predicciones están a 1.33 unidades de las reales, y no penaliza los errores grandes de manera tan severa como el MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparámetros más frecuentemente ajustados:\n",
    "\n",
    "learning_rate: también llamado eta, especifica qué tan rápido el modelo ajusta los errores residuales utilizando aprendices base adicionales.\n",
    "Valores típicos: 0.01–0.2\n",
    "\n",
    "__gamma, reg_alpha, reg_lambda__: estos 3 parámetros especifican los valores para 3 tipos de regularización que realiza XGBoost: reducción mínima de la pérdida para crear una nueva división, regularización L1 en los pesos de las hojas, y regularización L2 en los pesos de las hojas, respectivamente.\n",
    "\n",
    "Valores típicos para gamma: 0 - 0.5, pero muy dependiente de los datos. Valores típicos para reg_alpha y reg_lambda: 0 - 1 es un buen punto de partida, pero nuevamente, depende de los datos.\n",
    "\n",
    "__max_depth__: la profundidad máxima que pueden tener los nodos de decisión del árbol. Debe ser un número entero positivo.\n",
    "Valores típicos: 1–10\n",
    "\n",
    "__subsample__: fracción del conjunto de entrenamiento que puede ser utilizada para entrenar cada árbol. Si este valor es bajo, puede llevar a un ajuste insuficiente (underfitting); si es demasiado alto, puede llevar a un sobreajuste (overfitting).\n",
    "Valores típicos: 0.5–0.9\n",
    "\n",
    "__colsample_bytree__: fracción de las características que se pueden usar para entrenar cada árbol. Un valor alto significa que casi todas las características pueden ser usadas para construir el árbol de decisión.\n",
    "Valores típicos: 0.5–0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el modelo en un archivo .json\n",
    "modelo_xgbregresion.save_model('modelo_XGBRegressor.json')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
